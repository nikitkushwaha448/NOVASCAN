# ğŸš€ NOVASCAN  
**Discover startup opportunities hidden in social conversations**

Built for **AI Accelerate: Unlocking New Frontiers** â€” helping entrepreneurs validate ideas and discover real problems *before* building.

---

## ğŸŒ What is NOVASCAN?

**NOVASCAN** analyzes thousands of real discussions from **Reddit, Hacker News, YouTube, and Product Hunt** to help you:

- ğŸ” Discover trending problems people are struggling with
- ğŸ“Š Validate startup ideas using real market demand data
- ğŸ¯ Find early adopters before you build
- ğŸ“ˆ Analyze market opportunities with AI-powered insights

Instead of guessing ideas, NOVASCAN shows you **what people are already talking about, struggling with, and paying for**.

---

## âœ¨ Key Features

### ğŸ¤– AI Idea Validation
Enter any startup idea and get instant validation backed by real social discussions:

- Market Demand Score (0â€“100)
- Problem Severity Analysis
- Competition Level Assessment
- Monetization Potential
- Target User Identification
- Final Verdict: **BUILD IT / MAYBE / DON'T BUILD**

---

### ğŸ“Š Elasticsearch Analytics Dashboard
Advanced aggregations showing:

- **Trend Over Time** â€“ Discussion volume by hour/day
- **Platform Breakdown** â€“ Where conversations happen most
- **Sentiment Distribution** â€“ Positive / Neutral / Negative
- **Peak Activity Hours** â€“ Best posting time (timezone-aware)
- **Top Posts by Engagement** â€“ Most valuable discussions

---

### ğŸ” AI-Powered Reranking
Using **Elastic Open Inference API + Vertex AI**:

- **Stage 1:** BM25 + vector search â†’ Top 100 results
- **Stage 2:** Vertex AI semantic reranking â†’ Top 20 results

âœ… Result: **95% relevance** vs ~70% with standard search

---

### ğŸ’¬ Grounded AI Chat
Ask questions about search results with:

- Answers grounded in real discussions
- Live citations to source posts
- Conversational follow-ups

No hallucinations â€” only real data.

---

## ğŸ§  Innovation Highlights

### ğŸ”€ Elasticsearch Hybrid Search
- Combines **BM25 keyword matching** with **dense vector similarity**
- Uses **Vertex AI text-embedding-004**

---

### ğŸ“ Advanced Elasticsearch Aggregations
Showcases:

- `date_histogram` for time-series trends
- `terms` aggregations for categories
- `avg` metrics for statistics
- `painless` scripting for custom hour extraction
- `top_hits` for document sampling

---

### âš¡ Elastic Open Inference API
- Connects **Elasticsearch â†’ Vertex AI**
- Uses `semantic-ranker-512@latest`
- Implements `text_similarity_reranker`
- Two-stage retrieval for speed + relevance

---

### ğŸ§¬ Multi-Model Vertex AI Strategy

| Model | Purpose |
|-----|--------|
| Gemini 2.5 Flash | Fast idea validation & chat |
| Gemini 2.5 Pro | Deep opportunity analysis |
| text-embedding-004 | 768-dim semantic embeddings |
| semantic-ranker-512 | AI-powered reranking |

Cost-optimized: Flash handles volume, Pro handles depth.

---

## ğŸ›  Tech Stack

| Category | Technologies |
|-------|-------------|
| Search & Database | Elasticsearch 8.14+ |
| AI & ML | Vertex AI (Gemini 2.5 Flash/Pro) |
| Backend | Next.js 14 API Routes |
| Frontend | Next.js 14, React, TailwindCSS, TypeScript |
| Data Sources | Reddit, Hacker News, YouTube, Product Hunt |
| Deployment | Vercel, Elasticsearch Cloud |

---

## âš¡ Quick Start

### âœ… Prerequisites
- Node.js **18.17+**
- Google Cloud account with **Vertex AI enabled**
- Elasticsearch Cloud account
- Google Cloud CLI installed

---

### 1. Clone & Install

```bash
git clone <your-repo-url>
cd StartupRadar
npm install
```

### 2. Set Up Environment Variables

Create `.env`:

```bash

# Google Cloud / Vertex AI
GOOGLE_CLOUD_PROJECT_ID="your-project-id"
GOOGLE_CLOUD_LOCATION="us-central1"
GOOGLE_APPLICATION_CREDENTIALS="<service-account-json or base64>"

# Elasticsearch
ELASTIC_CLOUD_ID="your-cloud-id"
ELASTIC_API_KEY="your-api-key"

# Reddit, ProductHunt, HackerNews and Youtube API Keys
REDDIT_CLIENT_ID=""
REDDIT_USER_AGENT=""
REDDIT_CLIENT_SECRET=""
REDDIT_USER_AGENT=""
PRODUCTHUNT_CLIENT_ID=""
PRODUCTHUNT_CLIENT_SECRET=""
PRODUCTHUNT_API_TOKEN=""
HACKERNEWS_API_URL=""
YOUTUBE_API_KEY=""

```

### 3. Authenticate with Google Cloud

```bash
gcloud auth application-default login
gcloud config set project your-project-id
```

### 4. Set Up Elasticsearch Index

```bash
npm run setup-es
```

This creates the `social_signals` index with mappings for:

- Dense vectors (768 dimensions)
- Sentiment analysis fields
- Quality metrics
- Platform metadata

### 5. (Optional) Enable AI Reranking

Prerequisites:

1. Enable Discovery Engine API in Google Cloud Console
2. Grant Discovery Engine Viewer role to service account
3. Wait 2-3 minutes for API propagation

```bash
npm run setup-reranking
```

This creates the Vertex AI reranking inference endpoint using Elastic's Open Inference API.

### 6. Collect Initial Data

```bash
npm run collect-data
```

Fetches ~100 posts from Reddit, Hacker News, YouTube, Product Hunt and indexes with embeddings. Takes about 5-10 minutes.

### 7. Run the App

```bash
npm run dev
```

Open http://localhost:3000

## Example Use Cases

### Discover Problems

```
Search: "struggling with meeting notes"
â†’ Finds 60 discussions about meeting note pain points
â†’ Shows trend: Rising 40% in last 30 days
â†’ Identifies: Remote teams, managers, consultants
```

### Validate Ideas

```
Idea: "AI-powered meeting notes with action item extraction"
â†’ Market Demand: 85/100 (Strong)
â†’ Willingness to Pay: 72/100 (High)
â†’ Verdict: BUILD IT
â†’ Recommendation: Focus on async teams, $20/mo SaaS
```

### Find Early Adopters

```
Opportunity Analysis â†’ Early Adopters Section
â†’ Shows top 10 users discussing the problem
â†’ Platforms: r/Entrepreneur, r/startups, Hacker News
â†’ Engagement levels: 50+ upvotes, 20+ comments
```

## Project Structure

```
NOVASCAN/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ analytics/route.ts      # Analytics aggregations
â”‚   â”‚   â”œâ”€â”€ analyze-opportunity/    # AI opportunity analysis
â”‚   â”‚   â”œâ”€â”€ validate-idea/          # AI idea validation
â”‚   â”‚   â”œâ”€â”€ chat/route.ts           # Grounded AI chat
â”‚   â”‚   â””â”€â”€ search/route.ts         # Hybrid search + reranking
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ AnalyticsDashboard.tsx  # Visual analytics (charts)
â”‚   â”‚   â”œâ”€â”€ OpportunityReport.tsx   # Opportunity analysis UI
â”‚   â”‚   â””â”€â”€ SearchResults.tsx       # Search results display
â”‚   â”œâ”€â”€ dashboard/page.tsx          # Main search interface
â”‚   â”œâ”€â”€ validate/page.tsx           # Idea validation page
â”‚   â””â”€â”€ page.tsx                    # Landing page
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ embeddings.ts           # Vertex AI text-embedding-004
â”‚   â”‚   â”œâ”€â”€ grounding.ts            # Vertex AI grounded chat
â”‚   â”‚   â”œâ”€â”€ idea-validator.ts       # AI idea validation logic
â”‚   â”‚   â””â”€â”€ opportunity-analyzer.ts # AI opportunity scoring
â”‚   â”œâ”€â”€ elasticsearch/
â”‚   â”‚   â”œâ”€â”€ client.ts               # ES client setup
â”‚   â”‚   â”œâ”€â”€ search.ts               # Hybrid search + reranking
â”‚   â”‚   â”œâ”€â”€ analytics.ts            # Advanced aggregations
â”‚   â”‚   â””â”€â”€ reranking.ts            # Open Inference API setup
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ reddit.ts               # Reddit API
â”‚   â”‚   â”œâ”€â”€ hackernews.ts           # Hacker News API
â”‚   â”‚   â”œâ”€â”€ youtube.ts              # YouTube API
â”‚   â”‚   â””â”€â”€ producthunt.ts          # Product Hunt API
â”‚   â””â”€â”€ types/index.ts              # TypeScript interfaces
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-elasticsearch.ts      # Create ES index
â”‚   â”œâ”€â”€ setup-reranking.ts          # Setup reranking endpoint
â”‚   â””â”€â”€ collect-data.ts             # Data collection job
â””â”€â”€ package.json
```

## Challenges I Faced

- **Deduplication** turned into a bigger problem than expected. The same discussion often appears multiple times, sometimes with slight title variations or different URLs. I built a normalization system that strips protocols and URL parameters, normalizes titles by removing punctuation and truncating to 100 characters. Even then, some duplicates slip through when titles are significantly reworded.

- **API rate limits** hit hard once I started scaling data collection. YouTube's quota system is particularly brutalâ€”you burn through your daily limit fast. Reddit blocks you if you make requests too quickly. I added delays between requests (1 second for Reddit, 2 seconds for embedding batches) and implemented `Promise.allSettled` so one platform failure doesn't kill the entire collection job.

- **Platform-specific quirks** created a normalization nightmare. **Product Hunt** uses GraphQL with nested topic structures (`topics.edges[].node.name`), **Reddit** has inconsistent formats where `selftext` might be empty and timestamps are in Unix seconds, **YouTube** requires two API calls (search, then fetch details) with engagement metrics as strings needing parsing, and **HackerNews** has two separate APIs (Firebase and Algolia) with stories that can be `deleted` or `dead`. I solved this with a unified **`SocialPost` interface** that normalizes everythingâ€”YouTube's `likeCount` and Reddit's `score` both map to a generic `score` field, all timestamp formats convert to JavaScript `Date` objects, and each connector has a `normalize` function.

- **The background collection pipeline** needed careful orchestration. Fetching from four platforms, running sentiment analysis, calculating quality scores, generating embeddings, and bulk indexingâ€”all while handling partial failures gracefully. I use `Promise.allSettled` everywhere, so one platform timeout doesn't break the entire job.

## What's Next

- **Validation API performance** - The `/api/validate-idea` endpoint takes nearly a minute to complete. The main bottleneck is sequential Gemini calls and Elasticsearch searches. **Solution: Multi-layer caching.** Cache Elasticsearch results for identical search queries (1-hour TTL), cache generated embeddings for common keywords, and cache entire validation reports for identical ideas (6-hour TTL). This could drop response time from 60 seconds to under 2 seconds for cache hits. Redis with query hash keys would handle this cleanly.

- **Authentication & rate limiting** - Currently everything is public with no usage tracking. Adding NextAuth or Clerk would enable user accounts, and token bucket rate limiting per user would prevent abuse.

- **Better deduplication** - The current URL/title normalization still lets duplicates through when titles are significantly reworded. A similarity-based approach using embeddings could catch near-duplicates more effectively.

## Roadmap

- [ ] Add more platforms (X, GitHub, Stackoverflow, Quora)
- [ ] Multi-layer caching (Redis) for validation API
- [ ] Authentication system (NextAuth/Clerk)
- [ ] Add export functionality (CSV, PDF reports)
- [ ] Multi-lingual support
- [ ] Browser extension for on-the-fly validation

## Acknowledgments

Built with:

- Elasticsearch - For powerful hybrid search and aggregations
- Google Cloud Vertex AI - For embeddings, reranking, and Gemini models
- Next.js - For the amazing developer experience

---

Made for the AI Accelerate: Unlocking New Frontiers

Find problems worth solving. Build startups people want.


